{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Tokenization using NLTK:"
      ],
      "metadata": {
        "id": "Oa7o7TntUpZq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAeDGXIATbNG",
        "outputId": "d6c0d260-46e4-40e3-80b1-efee5204ed84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens:\n",
            "['Natural', 'Language', 'Processing', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.', 'Text', 'Classification', ':', 'Automatically', 'categorizing', 'text', 'documents', 'into', 'predefined', 'categories', '.']\n",
            "\n",
            "Sentence Tokens:\n",
            "['Natural Language Processing is a branch of artificial intelligence that focuses on the interaction between computers and humans through natural language.', 'Text Classification: Automatically categorizing text documents into predefined categories.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"Natural Language Processing is a branch of artificial intelligence that focuses on the interaction between computers and humans through natural language. Text Classification: Automatically categorizing text documents into predefined categories.\"\n",
        "# Word Tokenization\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word Tokens:\")\n",
        "print(word_tokens)\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "print(\"\\nSentence Tokens:\")\n",
        "print(sentence_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Stop Words Removal using NLTK:"
      ],
      "metadata": {
        "id": "cM7G6NauUyjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"Stop words are common words that are often filtered out during the preprocessing of text data because they do not carry significant meaning.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Load English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Filtered Tokens after Stop Words Removal:\")\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqbEalgsTtSC",
        "outputId": "c03f30f5-e177-4ce1-9f99-f47734403f8c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens after Stop Words Removal:\n",
            "['Stop', 'words', 'common', 'words', 'often', 'filtered', 'preprocessing', 'text', 'data', 'carry', 'significant', 'meaning', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: Stemming and Lemmatization using NLTK:"
      ],
      "metadata": {
        "id": "LfZnXGW7U-1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "text = \"Stemming and lemmatization are both techniques used in NLP for reducing words to their base or root form.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Initialize Stemmer and Lemmatizer\n",
        "porter_stemmer = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Perform Stemming\n",
        "stemmed_words = [porter_stemmer.stem(word) for word in tokens]\n",
        "\n",
        "print(\"Stemmed Words:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "# Perform Lemmatization\n",
        "lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "print(\"\\nLemmatized Words:\")\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5BEDO_FTt5i",
        "outputId": "d542fa17-c3fd-48d5-f7db-82a92e100fe1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words:\n",
            "['stem', 'and', 'lemmat', 'are', 'both', 'techniqu', 'use', 'in', 'nlp', 'for', 'reduc', 'word', 'to', 'their', 'base', 'or', 'root', 'form', '.']\n",
            "\n",
            "Lemmatized Words:\n",
            "['Stemming', 'and', 'lemmatization', 'are', 'both', 'technique', 'used', 'in', 'NLP', 'for', 'reducing', 'word', 'to', 'their', 'base', 'or', 'root', 'form', '.']\n"
          ]
        }
      ]
    }
  ]
}