{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa7o7TntUpZq"
      },
      "source": [
        "# Task 2: Tokenization using NLTK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAeDGXIATbNG",
        "outputId": "aafbbb63-3747-49f4-88a7-2a582810d092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens:\n",
            "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'a', 'text', 'into', 'smaller', 'units', ',', 'typically', 'words', 'or', 'sentences', ',', 'called', 'tokens', '.', 'In', 'NLP', ',', 'tokenization', 'is', 'a', 'fundamental', 'step', 'in', 'preprocessing', 'text', 'data', '.']\n",
            "\n",
            "Sentence Tokens:\n",
            "['Tokenization is the process of breaking down a text into smaller units, typically words or sentences, called tokens.', 'In NLP, tokenization is a fundamental step in preprocessing text data.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"Tokenization is the process of breaking down a text into smaller units, typically words or sentences, called tokens. In NLP, tokenization is a fundamental step in preprocessing text data.\"\n",
        "\n",
        "# Word Tokenization\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word Tokens:\")\n",
        "print(word_tokens)\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "print(\"\\nSentence Tokens:\")\n",
        "print(sentence_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM7G6NauUyjx"
      },
      "source": [
        "# Task 3: Stop Words Removal using NLTK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqbEalgsTtSC",
        "outputId": "be7e160a-2544-40ad-e201-6a62bac49ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens after Stop Words Removal:\n",
            "['Tokenization', 'process', 'breaking', 'text', 'smaller', 'units', ',', 'typically', 'words', 'sentences', ',', 'called', 'tokens', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"Tokenization is the process of breaking down a text into smaller units, typically words or sentences, called tokens.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Load English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Filtered Tokens after Stop Words Removal:\")\n",
        "print(filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfZnXGW7U-1q"
      },
      "source": [
        "# Task 4: Stemming and Lemmatization using NLTK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5BEDO_FTt5i",
        "outputId": "07e328bc-85e8-48c6-9632-e19d26f760e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words:\n",
            "['token', 'is', 'the', 'process', 'of', 'break', 'down', 'a', 'text', 'into', 'smaller', 'unit', ',', 'typic', 'word', 'or', 'sentenc', ',', 'call', 'token', '.']\n",
            "\n",
            "Lemmatized Words:\n",
            "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'a', 'text', 'into', 'smaller', 'unit', ',', 'typically', 'word', 'or', 'sentence', ',', 'called', 'token', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "text = \"Tokenization is the process of breaking down a text into smaller units, typically words or sentences, called tokens.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Initialize Stemmer and Lemmatizer\n",
        "porter_stemmer = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Perform Stemming\n",
        "stemmed_words = [porter_stemmer.stem(word) for word in tokens]\n",
        "\n",
        "print(\"Stemmed Words:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "# Perform Lemmatization\n",
        "lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "print(\"\\nLemmatized Words:\")\n",
        "print(lemmatized_words)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}