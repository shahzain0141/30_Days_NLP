{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DmqQdP8hffC2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Data Preprocessing\n",
        "\n",
        "english_sentences = ['start hello end', 'start world end', 'start how are you end', 'start I am fine end', 'start have a good day end']\n",
        "spanish_sentences = ['start hola end', 'start mundo end', 'start cómo estás end', 'start estoy bien end', 'start ten un buen día end']\n",
        "\n",
        "source_tokenizer = Tokenizer()\n",
        "source_tokenizer.fit_on_texts(english_sentences)\n",
        "source_sequences = source_tokenizer.texts_to_sequences(english_sentences)\n",
        "input_texts = pad_sequences(source_sequences, padding='post')\n",
        "\n",
        "target_tokenizer = Tokenizer()\n",
        "target_tokenizer.fit_on_texts(spanish_sentences)\n",
        "target_sequences = target_tokenizer.texts_to_sequences(spanish_sentences)\n",
        "target_texts = pad_sequences(target_sequences, padding='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate\n",
        "from tensorflow.keras.layers import AdditiveAttention as Attention\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Model parameters\n",
        "embedding_dim = 256\n",
        "latent_dim = 512\n",
        "num_encoder_tokens = len(source_tokenizer.word_index) + 1\n",
        "num_decoder_tokens = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(num_encoder_tokens, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(num_decoder_tokens, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "# Attention Layer\n",
        "attention = Attention()\n",
        "attention_output = attention([decoder_outputs, encoder_outputs])\n",
        "\n",
        "# Concatenating attention output and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_output])\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)"
      ],
      "metadata": {
        "id": "j7YHVB99sD1r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "w3kg39EGsRqd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "decoder_target_data = to_categorical(target_texts, num_decoder_tokens)\n",
        "model.fit([input_texts, target_texts], decoder_target_data, batch_size=64, epochs=50, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqlefWKpsU_g",
        "outputId": "dcdcb833-c94c-4f50-e71f-2ecb4c45daf4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 2.5697 - accuracy: 0.0417 - val_loss: 2.5720 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 2.4592 - accuracy: 0.4167 - val_loss: 2.5778 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 2.3456 - accuracy: 0.4167 - val_loss: 2.5890 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 2.1826 - accuracy: 0.4167 - val_loss: 2.6197 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 1.9245 - accuracy: 0.4167 - val_loss: 2.7407 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 1.6391 - accuracy: 0.4167 - val_loss: 2.8185 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 1.5187 - accuracy: 0.4167 - val_loss: 2.8343 - val_accuracy: 0.1667\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 1.4276 - accuracy: 0.5833 - val_loss: 3.0689 - val_accuracy: 0.1667\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 1.3532 - accuracy: 0.5833 - val_loss: 2.8753 - val_accuracy: 0.1667\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 1.3263 - accuracy: 0.6667 - val_loss: 3.9302 - val_accuracy: 0.1667\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 1.4060 - accuracy: 0.5833 - val_loss: 2.7942 - val_accuracy: 0.1667\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 1.4532 - accuracy: 0.6667 - val_loss: 3.6887 - val_accuracy: 0.1667\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 1.2629 - accuracy: 0.5833 - val_loss: 3.1494 - val_accuracy: 0.1667\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 1.1557 - accuracy: 0.6667 - val_loss: 3.6140 - val_accuracy: 0.1667\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 342ms/step - loss: 1.0910 - accuracy: 0.5833 - val_loss: 3.5907 - val_accuracy: 0.1667\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 1.0418 - accuracy: 0.6667 - val_loss: 4.0449 - val_accuracy: 0.1667\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.9956 - accuracy: 0.6667 - val_loss: 3.9796 - val_accuracy: 0.1667\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.9574 - accuracy: 0.7500 - val_loss: 4.9253 - val_accuracy: 0.1667\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 0.9533 - accuracy: 0.6667 - val_loss: 4.0098 - val_accuracy: 0.1667\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 1.0668 - accuracy: 0.6667 - val_loss: 6.2549 - val_accuracy: 0.1667\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 1.3486 - accuracy: 0.5833 - val_loss: 3.8157 - val_accuracy: 0.1667\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 376ms/step - loss: 0.9397 - accuracy: 0.7500 - val_loss: 4.8988 - val_accuracy: 0.1667\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.8462 - accuracy: 0.6667 - val_loss: 4.8956 - val_accuracy: 0.1667\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.8032 - accuracy: 0.7500 - val_loss: 5.4788 - val_accuracy: 0.1667\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.7696 - accuracy: 0.6667 - val_loss: 5.5709 - val_accuracy: 0.1667\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.7395 - accuracy: 0.7500 - val_loss: 6.2649 - val_accuracy: 0.1667\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.7179 - accuracy: 0.6667 - val_loss: 6.0094 - val_accuracy: 0.1667\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.7168 - accuracy: 0.6667 - val_loss: 7.3492 - val_accuracy: 0.1667\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.8136 - accuracy: 0.6667 - val_loss: 5.5340 - val_accuracy: 0.1667\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.9287 - accuracy: 0.6667 - val_loss: 7.6149 - val_accuracy: 0.1667\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 1.0856 - accuracy: 0.5833 - val_loss: 5.0481 - val_accuracy: 0.1667\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.6899 - accuracy: 0.8333 - val_loss: 6.3142 - val_accuracy: 0.1667\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.6200 - accuracy: 0.7500 - val_loss: 6.3091 - val_accuracy: 0.1667\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.5870 - accuracy: 0.8333 - val_loss: 6.8463 - val_accuracy: 0.1667\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.5598 - accuracy: 0.8333 - val_loss: 6.9584 - val_accuracy: 0.1667\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5340 - accuracy: 0.8333 - val_loss: 7.4370 - val_accuracy: 0.1667\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5097 - accuracy: 0.9167 - val_loss: 7.3890 - val_accuracy: 0.1667\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4871 - accuracy: 0.9167 - val_loss: 8.1086 - val_accuracy: 0.1667\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4720 - accuracy: 0.8333 - val_loss: 7.3350 - val_accuracy: 0.1667\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4820 - accuracy: 1.0000 - val_loss: 9.5388 - val_accuracy: 0.1667\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6192 - accuracy: 0.7917 - val_loss: 5.9434 - val_accuracy: 0.1667\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.8184 - accuracy: 0.7083 - val_loss: 9.6711 - val_accuracy: 0.1667\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.8914 - accuracy: 0.5833 - val_loss: 6.3222 - val_accuracy: 0.1667\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5060 - accuracy: 0.8333 - val_loss: 7.7067 - val_accuracy: 0.1667\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.4219 - accuracy: 0.8333 - val_loss: 7.5367 - val_accuracy: 0.1667\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.3861 - accuracy: 0.9583 - val_loss: 8.0888 - val_accuracy: 0.1667\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.3669 - accuracy: 0.9167 - val_loss: 8.2345 - val_accuracy: 0.1667\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.3500 - accuracy: 0.9167 - val_loss: 8.6279 - val_accuracy: 0.1667\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.3349 - accuracy: 0.9167 - val_loss: 8.7551 - val_accuracy: 0.1667\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.3228 - accuracy: 0.9583 - val_loss: 9.1583 - val_accuracy: 0.1667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78c73d8be920>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Uf5HqcSponj2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a105d46-a05f-4ff4-c972-cbe116ed7d49"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)     (None, None, 256)            3840      ['input_5[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)     (None, None, 256)            3328      ['input_6[0][0]']             \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)               [(None, 512),                1574912   ['embedding_4[0][0]']         \n",
            "                              (None, 512),                                                        \n",
            "                              (None, 512)]                                                        \n",
            "                                                                                                  \n",
            " lstm_5 (LSTM)               [(None, None, 512),          1574912   ['embedding_5[0][0]',         \n",
            "                              (None, 512),                           'lstm_4[0][1]',              \n",
            "                              (None, 512)]                           'lstm_4[0][2]']              \n",
            "                                                                                                  \n",
            " additive_attention_2 (Addi  (None, None, 512)            512       ['lstm_5[0][0]',              \n",
            " tiveAttention)                                                      'lstm_4[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, None, 1024)           0         ['lstm_5[0][0]',              \n",
            " )                                                                   'additive_attention_2[0][0]']\n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, None, 13)             13325     ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3170829 (12.10 MB)\n",
            "Trainable params: 3170829 (12.10 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}